---
title: "Report 1"
author: "Kamil Zaborniak"
output: pdf_document
header-includes:
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage[table]{xcolor}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
 \usepackage{multicol}
---
\newtheorem{example}{Przyk≈Çad}[section]

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#library(reshape2)
library(goftest)
library(data.table)
library(rmarkdown)
library(kableExtra)
library(gridExtra)
library(stringr)
library(tibble)
library(ggplot2)
library(grid)
library(egg)
library(dplyr)
library(readr)
library(MASS)
library(latex2exp)
library(multtest)
library(mutoss)
library(plyr)
library(ellipse)
library(magrittr)
```

\tableofcontents


\pagebreak


# Vector and Matrix Algebra

Let's consider the following symmetric matrix:

$$A=
\begin{bmatrix}
3 & -1\\
-1 & 3\\
\end{bmatrix}.$$
Let's perform the spectral decomposition of \textbf{A}.
Let's start by determining the roots of the characteristic polynomial, i.e. the eigenvalues of the above matrix.
$$
\begin{vmatrix}
3-\lambda & -1\\
-1 & 3-\lambda\\
\end{vmatrix} =
(3-\lambda)^2-1=
(2-\lambda)(4-\lambda).
$$
So we got eigenvalues: $\lambda_1=2$, $\lambda_2=4$.
To determine the eigenvectors of the matrix: $e_1=[x_1, y_1]^T$, $e_2=[x_2, y_2]^T$ , let us solve the systems of equations:
\begin{multicols}{2}
\begin{align*}
\text{\textbf{A}}e_1=2e_1
\end{align*}
\begin{align*}
    &\begin{cases}
      3x_1-y_1=2x_1\\
      -x_1+3y_1=2y_1
    \end{cases}\\
    &\begin{cases}
      x_1=\tfrac{1}{\sqrt{2}}\\
      y_1=\tfrac{1}{\sqrt{2}}
    \end{cases}
\end{align*}
\begin{align*}
e_1=\begin{bmatrix}
\tfrac{1}{\sqrt{2}}\\
\tfrac{1}{\sqrt{2}}
\end{bmatrix}.
\end{align*}

\columnbreak
\begin{align*}
\text{\textbf{A}}e_2=4e_2
\end{align*}
\begin{align*}
    &\begin{cases}
      3x_2-y_2=4x_2\\
      -x_2+3y_2=4y_2
    \end{cases}\\
    &\begin{cases}
      x_2=\tfrac{1}{\sqrt{2}}\\
      y_2=-\tfrac{1}{\sqrt{2}}
    \end{cases}
\end{align*}
\begin{align*}
e_2=\begin{bmatrix}
\tfrac{1}{\sqrt{2}}\\
-\tfrac{1}{\sqrt{2}}
\end{bmatrix}.
\end{align*}

\end{multicols}
The spectral decomposition of A has form:
$$A=P\Lambda P^T=
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}
\begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}.\\$$
We see that the columns of matrix P are subsequent eigenvectors of matrix A: $P=[e_1,e_2]$, what is more $\Lambda= diag(\lambda_1, \lambda_2)$.

We can also write matrix \textbf{A} as:
\begin{equation*}
\begin{split}
\lambda_1e_1e_1^T+\lambda_2e_2e_2^T & =\tfrac{1}{2}
\left( 2 \begin{bmatrix}
1\\
1\\
\end{bmatrix}[1\quad 1]+ 4 
\begin{bmatrix}
1\\
-1\\
\end{bmatrix} [1\quad -1]
\right)\\ 
& =
\frac{1}{2}\left(
\begin{bmatrix}
2 & 2\\
2 & 2\\
\end{bmatrix}+
\begin{bmatrix}
4 & -4\\
-4 & 4\\
\end{bmatrix}
\right)\\
&=
\frac{1}{2} \left( \begin{bmatrix}
6 & -2\\
-2 & 6\\
\end{bmatrix}
\right)\\
& =
\begin{bmatrix}
3 & -1\\
-1 & 3\\
\end{bmatrix}.
\end{split}
\end{equation*}

To find $\sqrt{A}$, we use the spectral decomposition of A:
\begin{equation*}
\begin{split}
\sqrt{A}&=P\sqrt{\Lambda}P^T\\
&=\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}
\begin{bmatrix}
\sqrt{2} &  0\\
0 & 2
\end{bmatrix}
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}\\
&=
\begin{bmatrix}
\tfrac{1+\sqrt{2}}{2} &  \tfrac{\sqrt{2}-2}{2} \\
\tfrac{\sqrt{2}-2}{2} & \tfrac{1+\sqrt{2}}{2}
\end{bmatrix}.
\end{split}
\end{equation*}

Using spectral decomposition we can prove that $A= \sqrt{A}\sqrt{A}$:
$$
\sqrt{A}\sqrt{A}=
P\sqrt{\Lambda}P^TP\sqrt{\Lambda}P^T=
P\sqrt{\Lambda}\mathbb{I}\sqrt{\Lambda}P^T=
P\sqrt{\Lambda}\sqrt{\Lambda}P^T=
P\Lambda P^T=
A.
$$


# Properties of the spectral decomposition

Let's consider the spectral decomposition of a positive definite matrix as given in
 Lecture 1:
 $$A=P\Lambda P^T,$$
where the columns of P are made of eigenvectors $e_i$, $i = 1,\ldots,n$ ($P=[e_1,\ldots,e_n]$, $e_i=[e_{1,i},\ldots,e_{n,i}]^T$) and they are orthonormalized, i.e. their lengths are one and they are orthogonal (peripendicular) one to
 another. The diagonal matrix $\Lambda$ has the corresponding (positive) eigenvalues on the
 diagonal ($\Lambda=diag(\lambda_1,\ldots,\lambda_n)$).
 
 
Properties:
\begin{enumerate}

\item $P^T=P^{-1}$, because:

\begin{equation*}
\begin{split}
P^T &= P^{-1}\qquad | \cdot_R P\\
P^T P&= \mathbb{I}\\
\begin{bmatrix}
e_{1,1} & \cdots & e_{n,1} \\
\vdots & \ddots& \vdots \\
e_{1,n} & \cdots & e_{n,n}
\end{bmatrix}
\begin{bmatrix}
e_{1,1} & \cdots & e_{1,n} \\
\vdots & \ddots& \vdots \\
e_{n,1} & \cdots & e_{n,n}
\end{bmatrix}
&=\mathbb{I}\\
\begin{bmatrix}
e_1^Te_1 & e_1^Te_2& \cdots & e_1^Te_n \\
e_2^Te_1 & e_2^Te_2 & \cdots & e_2^Te_n\\
\vdots & \vdots &  \ddots &  \vdots \\
e_n^Te_1 & e_n^Te_2 & \cdots & e_n^Te_n 
\end{bmatrix}&=\mathbb{I},
\end{split}
\end{equation*}
where $\forall_{i\in\{1,\ldots,n\}}\text{ }e_i^Te_i=1$ and $\forall_{i\in\{1,\ldots,n\}}\forall_{j:j\neq i}\text{ } e_i^Te_j=0$. 

For example:
\begin{equation*}
\begin{split}
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}^{-1}
=\frac{1}{-1}\begin{bmatrix}
-\tfrac{1}{\sqrt{2}} &  -\tfrac{1}{\sqrt{2}} \\
-\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}}
\end{bmatrix}=\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}^T.
\end{split}
\end{equation*}

\item $\det(\Lambda)=\prod_{i=1}^n\lambda_i$:
$$\det\left(\begin{bmatrix}
\lambda_1 & \cdots & 0 \\
\vdots & \ddots& \vdots \\
0 & \cdots & \lambda_n
\end{bmatrix} \right)=\lambda_1\det\left(  
\begin{bmatrix}
\lambda_2 & \cdots & 0 \\
\vdots & \ddots& \vdots \\
0 & \cdots & \lambda_n
\end{bmatrix}
\right)=
\lambda_1\lambda_2\det\left(  
\begin{bmatrix}
\lambda_3 & \cdots & 0 \\
\vdots & \ddots& \vdots \\
0 & \cdots & \lambda_n
\end{bmatrix}\right)=\cdots=\lambda_1\lambda_2\cdot\ldots\cdot\lambda_n
$$

For example:
$$
\det\left( \begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}\right)=2\cdot4-0\cdot0=8.
$$

\item $\det(A)=\det(\Lambda)$:

$$\det(A)=\det\left(P\Lambda P^T \right)=\det(P)\det(\Lambda)\det\left(P^T\right)=\det(\Lambda)\det\left( PP^T\right)=
\det(\Lambda\mathbb{I})=\det(\Lambda).$$

For example:
\begin{equation*}
\begin{split}
\det\left( \begin{bmatrix}
3 & -1\\
-1 & 3\\
\end{bmatrix} \right)&=\det\left(
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix} 
\right)\det\left( 
\begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}
\right)\det\left(\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix} \right)\\
&=
\det\left(\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix} \begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}\right)\det\left( \begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}\right)\\
&=\det(\mathbb{I})\det\left( \begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}\right)
\end{split}
\end{equation*}

\item Inverse matrix to $\Lambda$:
$$\Lambda^{-1}=\begin{bmatrix}
\lambda_1 &  \cdots&0\\
 \vdots& \ddots & \vdots\\
 0& \cdots& \lambda_n
\end{bmatrix}^{-1}=
\begin{bmatrix}
\tfrac{1}{\lambda_1} &  \cdots&0\\
 \vdots& \ddots & \vdots\\
 0& \cdots& \tfrac{1}{\lambda_n}.
\end{bmatrix}$$

For example:
$$\begin{bmatrix}
2 &  0\\
0 & 4
\end{bmatrix}^{-1}=
\begin{bmatrix}
\tfrac{1}{2} &  0\\
0 & \tfrac{1}{4}
\end{bmatrix}.
$$
\item A simple way to determine the inverse of a matrix A from its spectral decomposition is through $A^{-1}=P\Lambda^{-1}P^T$:
\begin{equation*}
\begin{split}
A^{-1}&=P\Lambda^{-1}P^T \qquad | \cdot_L A \\
AA^{-1}&=P\Lambda P^TP\Lambda^{-1}P^T \\
\mathbb{I}&=P\Lambda\mathbb{I}\Lambda^{-1}P^T\\
\mathbb{I}&=PP^T\\
\mathbb{I}&=\mathbb{I}.
\end{split}
\end{equation*}

For example:
$$
\begin{bmatrix}
3 &  -1\\
-1 & 3
\end{bmatrix}^{-1}= 
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}
\begin{bmatrix}
\tfrac{1}{2} &  0\\
0 & \tfrac{1}{4}
\end{bmatrix}
\begin{bmatrix}
\tfrac{1}{\sqrt{2}} &  \tfrac{1}{\sqrt{2}} \\
\tfrac{1}{\sqrt{2}} & -\tfrac{1}{\sqrt{2}}
\end{bmatrix}=
\begin{bmatrix}
\tfrac{3}{8} &  \tfrac{1}{8} \\
\tfrac{1}{8} &  \tfrac{3}{8}
\end{bmatrix}.
$$
Then:
$$\begin{bmatrix}
3 &  -1\\
-1 & 3
\end{bmatrix}
\begin{bmatrix}
\tfrac{3}{8} &  \tfrac{1}{8} \\
\tfrac{1}{8} &  \tfrac{3}{8}
\end{bmatrix}=\mathbb{I}.$$

\end{enumerate}


# The join distribution of length and weight of newborn children

 In a medical study, length $L$ and weight $W$ of newborn children is considered.
 It was assumed that $(L,W)$ will be modeled through a bivariate normal distribution. The following information has been known: the mean weight is 3343[g], with
 the standard deviation of 528[g], while the mean length is 49.8[cm], with the standard deviation of 2.5[cm]. Additionally the correlation between the length and the
 weight has been established and equal to 0.75.
 
 The join distribution of $(W,L)$ is bivariate normal, i.e. 
 $$\begin{bmatrix}
W\\
L
\end{bmatrix} \sim 
N\left(\begin{bmatrix}
3343\\
49.8
\end{bmatrix},
\begin{bmatrix}
278784& 990\\
990 & 6.25
\end{bmatrix}
\right).$$
The density of the above vector has form:
\begin{equation*}
\begin{split}
f\left(X\right)&=\frac{1}{2\pi}\det\left(
\begin{bmatrix}
278784& 990\\
990 & 6.25
\end{bmatrix}
\right)^{-\tfrac{1}{2}}
\exp\left(-\frac{1}{2}\left(X-\begin{bmatrix}
3343\\
49.8
\end{bmatrix}\right)^T
\begin{bmatrix}
278784& 990\\
990 & 6.25
\end{bmatrix}^{-1}
\left(X- \begin{bmatrix}
3343\\
49.8
\end{bmatrix} \right)
\right)
\\
&=\frac{\sqrt{7}}{4620\pi}\exp\left(-\frac{1}{2}\left(X-\begin{bmatrix}
3343\\
49.8
\end{bmatrix}\right)^T
\begin{bmatrix}
\tfrac{1}{121968}& \tfrac{-1}{770}\\
\tfrac{-1}{770} & \tfrac{64}{175}
\end{bmatrix}
\left(X- \begin{bmatrix}
3343\\
49.8
\end{bmatrix} \right)
\right)
\end{split}
\end{equation*}

Let's find the eigenvalues and eigenvectors of the covariance matrix $\Sigma$:

$$\det\left(\begin{bmatrix}
278784-\lambda& 990\\
990 & 6.25-\lambda
\end{bmatrix}  \right)=\lambda^2-278790.25\lambda+762300,$$
$\lambda^2-278790.25\lambda+762300=0$ for $\lambda_1=2.734$ and $\lambda_2=278787.516$. The eigenvectors are $e_1=[0.004,-1]^T$ and $e_2=[-1,-0.004]^T$.



```{r p3,3, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Elipses corresponding to the constant density contours of the join distribution W and L"}

sigma<- matrix(c(278784, 990,990, 6.25), nrow = 2)
mu<- c(3343,49.8)
# sigma: covariance matrix, mu: mean vector, level: probability 
ellipse_data_95 = ellipse(x = sigma, centre = mu, level = 0.05) %>% as.data.frame() %>% cbind(data.frame(prob=rep(0.05,100)))
ellipse_data_90 = ellipse(x = sigma, centre = mu, level = 0.10) %>% as.data.frame()%>% cbind(data.frame(prob=rep(0.10,100)))
ellipse_data_60 = ellipse(x = sigma, centre = mu, level = 0.30) %>% as.data.frame() %>% cbind(data.frame(prob=rep(0.30,100)))
ellipse_data_30 = ellipse(x = sigma, centre = mu, level = 0.90) %>% as.data.frame() %>% cbind(data.frame(prob=rep(0.90,100)))
data<- rbind(ellipse_data_95, ellipse_data_90, ellipse_data_60, ellipse_data_30)


diago <- eigen(sigma)
scaled_v <- diago$vectors %*% diag(sqrt(diago$values)) %>% as.data.frame()
e <-  ellipse::ellipse(sigma, npoints = 1000)


ggplot(data, aes(x, y, colour=prob, group=prob)) +
  geom_path() +
  geom_segment(aes(x=3343, y=49.8, xend=3343 +scaled_v[1,1], yend=49.8+scaled_v[2,1]), col="red", arrow = arrow(length=unit(.3, 'cm')))+
  geom_segment(aes(x=3343, y=49.8, xend=3343 +scaled_v[1,2], yend=49.8+scaled_v[2,2]), col="red", arrow = arrow(length=unit(.3, 'cm')))+
  theme_bw() 

```
The vectors in the above graph are perpendicular, but the differences in the scales of the x and y axes mean that we cannot see this.

The bivariate normal distribution is characterized by 5 parameters (2 means, 2 standard deviations and covariance between variables). The $p$-dimensional normal distribution is characterized by $p+p(p+1)/2$ ($p$ means, $p$ variances, $p(p-1)/2$ covariances).

Variable of length: L, is normal distributed: $L\sim N(49.8, 2.5)$.

The 3-$\sigma$ rule tells us that the variable $L$ belongs to the interval $[49.8-3\cdot2.5,49.8+3\cdot2.5 ]=[42.3,57.3]$ with a probability of 99.73\%.

When the child's weight is known ($W=4025$), the variable $L$ has a conditional normal distribution.

\begin{equation*}
\begin{split}
L|W &\sim N\left(49.8+ 2.5\tfrac{4025-3343}{528}\cdot 0.75, 6.25(1-\tfrac{9}{16})\right)\\
&\sim N\left(\tfrac{16711}{320}, \tfrac{175}{64})\right)
\end{split}
\end{equation*}
Now the 99.73\% CI for $L$ is $[44.019, 60.425]$.
Additional information about the child's weight should have narrowed the CI, but it did not do so in this case. The reason for this is that $L|W$ has a larger variance.

# Operations on a multivariate normal distribution

Let $X_1$, $X_2$, and $X_3$ be independent $N(\mu, \Sigma)$ random vectors of a dimension $p$. We have two vectors:
$$
V_1=\tfrac{1}{4}X_1-\tfrac{1}{2}X_2+ \tfrac{1}{4}X_3,
$$
$$
V_2=\tfrac{1}{4}X_1-\tfrac{1}{2}X_2- \tfrac{1}{4}X_3.
$$
These vectors has distributions:
$$V_1\sim N\left(\tfrac{1}{4}\mu-\tfrac{1}{2}\mu+\tfrac{1}{4}\mu, \tfrac{1}{16}\Sigma+\tfrac{1}{4}\Sigma+\tfrac{1}{16}\Sigma\right) =: N\left(0,\tfrac{3}{8} \Sigma\right), $$
$$V_2\sim N\left(\tfrac{1}{4}\mu-\tfrac{1}{2}\mu-\tfrac{1}{4}\mu, \tfrac{1}{16}\Sigma+\tfrac{1}{4}\Sigma+\tfrac{1}{16}\Sigma\right) =: N\left(-\tfrac{1}{2}\mu, \tfrac{3}{8}\Sigma\right). $$
The join distribution of $V=[V_1,V_2]^T$ is $2p$ dimensional multivariate normal distribution $N(\mu_V, \Sigma_V)$.
We see that:
$$V=\begin{bmatrix}
V_1\\
V_2
\end{bmatrix}=
\begin{bmatrix}
\tfrac{1}{4}&-\tfrac{1}{2} &\tfrac{1}{4}\\
\tfrac{1}{4}&-\tfrac{1}{2} &-\tfrac{1}{4}
\end{bmatrix}
\begin{bmatrix}
X_1\\
X_2\\
X_3
\end{bmatrix}.$$
So:
$$\mu_V= \begin{bmatrix}
\tfrac{1}{4}&-\tfrac{1}{2} &\tfrac{1}{4}\\
\tfrac{1}{4}&-\tfrac{1}{2} &-\tfrac{1}{4}
\end{bmatrix}
\begin{bmatrix}
\mu\\
\mu\\
\mu
\end{bmatrix}= 
\begin{bmatrix}
0\\
-\tfrac{1}{2}\mu
\end{bmatrix}
$$
$$\Sigma_V=
\begin{bmatrix}
\tfrac{1}{4}&-\tfrac{1}{2} &\tfrac{1}{4}\\
\tfrac{1}{4}&-\tfrac{1}{2} &-\tfrac{1}{4}
\end{bmatrix}
\begin{bmatrix}
\Sigma& 0 & 0\\
0 &\Sigma &0\\
0 & 0 & \Sigma
\end{bmatrix}
\begin{bmatrix}
\tfrac{1}{4}&\tfrac{1}{4} \\
-\tfrac{1}{2}&-\tfrac{1}{2} \\
\tfrac{1}{4}&-\tfrac{1}{4}
\end{bmatrix}=
\begin{bmatrix}
\tfrac{3}{8}\Sigma&\tfrac{1}{4}\Sigma\\
\tfrac{1}{4}\Sigma&\tfrac{3}{8}\Sigma 
\end{bmatrix}.
$$


# Analysis of weight and length data of newborn children

Healthcare providers and insurance companies aim to determine the necessary medical tests for newborns. They've devised a scoring system to identify whether a newborn requires special attention or if they're healthy and don't need extra care. This scoring system primarily relies on the newborn's weight and length, as these are reliable indicators of their health status.

Here's how the scoring works: If a newborn's weight or length is exceptionally high or low, falling outside the top or bottom 5% of typical values for newborns, they're given a score of zero. This suggests that they might need additional medical tests. If their measurements fall between the top 5% and top 25%, they receive a score of one, indicating they might need some extra attention. Otherwise, if their weight and length are within the range of what's typical for newborns (the bottom 75%), they're assigned a score of two, indicating good health.

To analyze this, we've gathered data from 736 recently born children in a specific region. While their records contain various information, we're particularly interested in their weight and length, which we'll use to determine how many newborns may require extra medical attention and how many are healthy without further tests. This data is extracted from the file \textit{WeightLength.txt}.


Let's assume vector $X=[W,L]^T$, where the variable $W$ determines the weight [g] and $L$ - the length [cm] of the child.. We know that vector $X$ comes from a two-dimensional normal distribution $N(\mu,\Sigma)$. The estimators of the parameters of this distribution are:

$$\hat{\mu}=[3233.545, 49.237]^T,$$
$$\hat{\Sigma}=\begin{bmatrix}
220276.658 & 915.296\\
915.296 & 4.443
\end{bmatrix}.$$

```{r, cache=TRUE, echo=FALSE, include=FALSE}
weightheight<- read.table("WeightLength.txt", header = T)
colMeans(weightheight) -> wl_means
cov(weightheight) -> wl_cov


```

```{r code p123, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, include=FALSE}

ggplot(weightheight, aes(x=Weight, y=Length))+
  geom_point(shape=1)+
  theme_bw() ->a1

ggplot(weightheight, aes(sample=Weight))+
  stat_qq(distribution = qnorm, dparams = list(mean=wl_means[1], sd=sqrt(wl_cov[1,1])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=wl_means[1], sd=sqrt(wl_cov[1,1])), col="red")+
  xlab("Theoretical quantiles of W")+
  ylab("Quantiles of sample of W")+
  theme_bw() -> a2

ggplot(weightheight, aes(sample=Length))+
  stat_qq(distribution = qnorm, dparams = list(mean=wl_means[2], sd=sqrt(wl_cov[2,2])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=wl_means[2], sd=sqrt(wl_cov[2,2])), col="red")+
  xlab("Theoretical quantiles of L")+
  ylab("Quantiles of Sample of L")+
  theme_bw() ->a3

grid.arrange(a2, a3, ncol = 2) -> a23

grid.arrange(a1, a23)

```
```{r p123, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Scatterplot and qq-plots for the marginal distributions of data."}
grid.arrange(a1, a23)
```
In the scaterrplot we see that the two-dimensional variables $X=[W,L]^T$ have a multivariate normal distribution and are positively correlated with each other.
Quantile-quantile charts prove that marginal distributions are normal distributions: $W\sim N(3233.545, 220276.658)$, $L\sim N(49.237, 4.443)$.

In order to assign points to children, we will use the construction . The kth child will receive 2 points if the corresponding vector $X_k=[Weight_k, Length_k]^T$ satisfies the inequality:
$$(X_k-\hat{\mu})^T\hat{\Sigma}^{-1}(X_k-\hat{\mu})\leq\chi_2^2(0.75).$$
Similarly, we will award point 1:
$$\chi^2_2(0.75)\leq(X_k-\hat{\mu})^T\hat{\Sigma}^{-1}(X_k-\hat{\mu})\leq\chi_2^2(0.95).$$
From the above it follows that the number of children who received 2 points is 541, 1 point: 157, 0 points: 38.
```{r code p134, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Scatterplot and qq-plots for the marginal distributions of data."}

ellipse_data_95 = ellipse(x = wl_cov, centre = wl_means , level = 0.95) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.95",100)))
ellipse_data_75 = ellipse(x = wl_cov, centre = wl_means , level = 0.75) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.75",100)))
data<- rbind(ellipse_data_95, ellipse_data_75)

ggplot(data, aes(x=Weight, y=Length, colour=prob, group=prob)) +
  geom_path() +
  theme_bw()+
  ylim(42.5, 55.1)+
  xlim(1922.5,4521)+
  theme( legend.position = "bottom") ->a1
  

```
```{r add score to data, cache=TRUE, echo=FALSE, include=FALSE}

weightheight %>% cbind( data.frame(score=rep(0,736))) -> weightheight

scoring <- function(x, mu, sigma){
  c<- as.matrix(x-mu) %*% solve(sigma) %*% t(as.matrix(x-mu))
  
  if(c<=qchisq(.75, 2)){return(2)}
  else if(c<=qchisq(.95, 2)){ return(1)}
  else{ return(0)}
}

for(i in 1:736){
  weightheight[i,]$score <- scoring(x=weightheight[i,1:2], mu= as.matrix(wl_means),
                                    sigma=as.matrix(wl_cov))
}

weightheight$score %<>% as.factor()

ggplot()+
  geom_point(data=weightheight, aes(x=Weight, y=Length, col=score), shape=1)+
  geom_path(data=data, aes(x=Weight, y=Length, group=prob))+
  theme_bw()+
  ylim(42.5, 55.1)+
  xlim(1922.5,4521)+
  theme( legend.position = "bottom") -> a2



```


```{r plot p124, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Plot of confidence ellipsoids and scatter plot with classification region for Weight and Lenght of newborn children.", fig.height=3}
grid.arrange(a1, a2, nrow=1)
#1asf
```




Let's find the spectral decomposition of the $\hat{\Sigma}$. The eigenvalues of the matrix $\hat{\Sigma}$ are $\lambda_1=0.64$ $\lambda_2=220280.5$ and their corresponding eigenvectors, respectively: $v_1=[0.004, -1]^T$, $v_2=[-1,-0.004]^T$. The spectral decomposition of $\hat{\Sigma}$ has form:
$$\hat{\Sigma}=P\Lambda P^T = 
\begin{bmatrix}
0.004 & -1 \\
-1 & -0.004
\end{bmatrix}
\begin{bmatrix}
0.64 & 0 \\
0 & 220280.5
\end{bmatrix}
\begin{bmatrix}
0.004 & -1 \\
-1 & -0.004
\end{bmatrix}$$

Let's transform our data by left-multiplying each vector $[W,L]$ by $P^T$.

```{r transformed data, cache=TRUE, echo=FALSE, include=FALSE}
eig<- eigen(wl_cov)
P<- eig$vectors[,2:1] %>% as.matrix()
data<- (t(P) %*% t(as.matrix(weightheight[,1:2]) ) ) %>% t() %>% as.data.frame()
data<- data[,2:1]
colnames(data)<- c("Weight","Length")
```
```{r code plot of transformed data, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, include=FALSE}

t(P) %*% as.matrix(wl_means)
cov(data)

ggplot(data, aes(x=Weight, y=Length))+
  geom_point(shape=1)+
  theme_bw() ->a1

ggplot(data, aes(sample=Weight))+
  stat_qq(distribution = qnorm, dparams = list(mean=-3233.72179, sd=sqrt(cov(data)[1,1])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=-3233.72179, sd=sqrt(cov(data)[1,1])), col="red")+
  xlab("Theoretical quantiles of transformed W")+
  ylab("Quantiles of sample of\n transformed W")+
  theme_bw() -> a2

ggplot(data, aes(sample=Length))+
  stat_qq(distribution = qnorm, dparams = list(mean=-35.80123, sd=sqrt(cov(data)[2,2])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=-35.80123, sd=sqrt(cov(data)[2,2])), col="red")+
  xlab("Theoretical quantiles of transformed L")+
  ylab("Quantiles of Sample of \n transformed L")+
  theme_bw() ->a3

ggarrange(a2, a3, ncol = 2) -> a23

grid.arrange(a1, a23)

```
```{r plot transformed data, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Scatterplot of transformed data.", fig.height=2}
a1
```
```{r plot2 transformed data, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Qq-plots for the marginal distributions of transformed data.", fig.height=2}
a23
```

We see that the data comes from a bivariate normal distribution (more precisely $N\left( P^T\hat{\mu}, P^T\hat{\Sigma} P \right)$ ). The transformation reduced the covariances between the variable $W$ and $L$, we can see in the scatterplot that the points do not accumulate along any straight line. Moreover, the transformation changed the values and the sign of the data.


# Analysis of weight and length data of newborn children and their parents' heights

Let's analyze the children's previous data with additional data including the heights of their mothers and fathers (from the file \textit{ParentsWeightLength.txt}). Let's also check what impact additional data has on children's scores.

```{r parents, cache=TRUE, echo=FALSE, include=FALSE}
wlp<- read.table("ParentsWeightLength.txt", header = T)
wlp<- wlp[,c(3,4,1,2)]
colMeans(wlp) -> wlp_means
cov(wlp) -> wlp_cov
#wl_means
#wlp_cov

```
Let the vector $X_i'=[W_i,L_i,F_i,M_i]^T$ correspond to the i-th record from the data set, where the variables W_i and L_i correspond, as before, to the weight and length of the i-th child, and $F_i$ and $ M_i$ correspond to the heights of the father and mother (respectively) of the ith child.

Vector $X_i'\sim N\left( \hat{\mu}', \hat{\Sigma}' \right)$, where:
$$\hat{\mu}'= \begin{bmatrix}
3233.545 \\
49.238 \\
177.416 \\
166.920
\end{bmatrix},
$$
$$\hat{\Sigma}'=\begin{bmatrix}
220276.658& 915.296&  931.859& 827.288& \\
915.296&   4.443&   3.290&  2.852 \\
931.859&  3.290&  12.612&  0.631\\
827.288&   2.852&   0.631&  9.7721065\\
\end{bmatrix}.$$

```{r code p22, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, include=FALSE}

ggplot(wlp, aes(x=Weight, y=FatherHeight))+
  geom_point(shape=1)+
  ylim(158, 189)+
  theme_bw() ->a1
ggplot(wlp, aes(x=Length, y=FatherHeight))+
  geom_point(shape=1)+
  ylim(158, 189)+
  theme_bw() ->a2

ggplot(wlp, aes(x=Weight, y=MotherHeight))+
  geom_point(shape=1)+
  ylim(158, 189)+
  theme_bw() ->a3
ggplot(wlp, aes(x=Length, y=MotherHeight))+
  geom_point(shape=1)+
  ylim(158, 189)+
  theme_bw() ->a4
ggplot(wlp, aes(x=FatherHeight, y=MotherHeight))+
  geom_point(shape=1)+
  theme_bw() ->a5

ggarrange(a1,a2,a3,a4, nrow=1)

ggplot(wlp, aes(sample=FatherHeight))+
  stat_qq(distribution = qnorm, dparams = list(mean=wlp_means[3], sd=sqrt(wlp_cov[3,3])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=wlp_means[3], sd=sqrt(wlp_cov[3,3])), col="red")+
  xlab("Theoretical quantiles of F")+
  ylab("Quantiles of\n sample of F")+
  theme_bw()+
  ylim(158, 189)+
  xlim(158, 189) -> a33

ggplot(wlp, aes(sample=MotherHeight ))+
  stat_qq(distribution = qnorm, dparams = list(mean=wlp_means[4], sd=sqrt(wlp_cov[4,4])))+
  stat_qq_line(distribution = qnorm, dparams = list(mean=wlp_means[4], sd=sqrt(wlp_cov[4,4])), col="red")+
  xlab("Theoretical quantiles of M")+
  ylab("Quantiles of\n Sample of M")+
  theme_bw()+
  ylim(158, 189)+
  xlim(158, 189) ->a44

grid.arrange(a1,a2,a3,a4, ncol = 4) -> a23

grid.arrange(a33,a44,nrow=1) -> a24


```
```{r sctr qq plot newdata, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Scatterplot of new data and qq-plots of Heights of parents."}
grid.arrange(a23, a5, a24, nrow=3)
```
The scatterplots above confirm that the data have a four-dimensional normal distribution.
The first 4 graphs show that there is a relationship between the child's metrics and the parent's height. The fifth chart shows that there is no relationship between the heights of both parents.
We also see that qq-plots confirm that adult height is also normally distributed.


The  conditional distribution of the weight and length of a child given the
 heights of parents has form $X|Y=y\sim N\left( \hat{\mu}^*, \hat{\Sigma}^*  \right)$, where $Y_i=[F_i, M_i]^T,$

\begin{equation*}
\begin{split}
\hat{\mu}^*&=
 \begin{bmatrix}
3233.545\\
49.238
\end{bmatrix}+
 \begin{bmatrix}
931.859& 827.288& \\
3.290&  2.852 \\
\end{bmatrix}
 \begin{bmatrix}
0.0795&  -0.005\\
-0.005&  0.103
\end{bmatrix}
\left(
y-
 \begin{bmatrix}
177.416\\
166.920
\end{bmatrix}
\right)\\
&=
\begin{bmatrix}
3233.545\\
49.238
\end{bmatrix}+
\begin{bmatrix}
69.876 &  80.146\\
0.247&   0.276
\end{bmatrix}
\left(
y-
 \begin{bmatrix}
177.416\\
166.920
\end{bmatrix}
\right)
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\hat{\Sigma}^*&=
\begin{bmatrix}
220276.658& 915.296 \\
915.296 &   4.443
\end{bmatrix}-
 \begin{bmatrix}
931.859& 827.288& \\
3.290&  2.852 \\
\end{bmatrix}
 \begin{bmatrix}
0.0795&  -0.005\\
-0.005&  0.103
\end{bmatrix}
 \begin{bmatrix}
931.859&   3.290\\
827.288&   2.852
\end{bmatrix}\\
&=
 \begin{bmatrix}
88857.996& 456.846\\
456.846&   2.844
\end{bmatrix}.
\end{split}
\end{equation*}



```{r code p24, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE}
ellipse_data_95 = ellipse(x = wl_cov, centre = wl_means , level = 0.95) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.95",100)))
ellipse_data_75 = ellipse(x = wl_cov, centre = wl_means , level = 0.75) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.75",100)))
dataa<- rbind(ellipse_data_95, ellipse_data_75)
dataa %<>% cbind(data.frame(CovMatrix=rep("previous", 200)))

wl_cov-wlp_cov[1:2,3:4]%*% solve(wlp_cov[3:4,3:4])%*% wlp_cov[3:4,1:2] -> sig_c

ellipse_data_95c = ellipse(x = sig_c, centre = wl_means , level = 0.95) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.95",100)))
ellipse_data_75c = ellipse(x = sig_c, centre = wl_means , level = 0.75) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.75",100)))
datac<- rbind(ellipse_data_95c, ellipse_data_75c) 

datac %<>% cbind(data.frame(CovMatrix=rep("current", 200)))

rbind(dataa, datac)-> d
ggplot(d, aes(x=Weight, y=Length, colour=prob, group=interaction(prob, CovMatrix))) +
  geom_path(aes(linetype= CovMatrix)) +
  theme_bw()+
  theme(legend.position = "bottom")+
  ylim(42.5, 55.1)+
  xlim(1922.5,4521)->a1
  

```



The diameters of the ellipses will be determined by the eigenvectors of the matrix $\hat{\Sigma}^*$, and the center of the ellipses will be the vector $\hat{\mu}^*$ . The ellipsoids based on the conditional distribution they are narrower and smaller, i.e. they have a smaller area compared to the previous ones.
The new ellipse has also been rotated. 

This makes sense because new data gives us more information about the distribution of given metrics. 

Let's assume $y=\hat{\mu}^*$, then the classification areas are shown by first below plot:

```{r p add score to data parents, cache=TRUE, echo=FALSE, include=FALSE}

wlp %>% cbind( data.frame(score=rep(0,736))) -> wlp

for(i in 1:736){
  mu_i= (as.matrix(wl_means)) + wlp_cov[1:2,3:4] %*% solve(wlp_cov[3:4,3:4]) %*% t(wlp[i,3:4] - wlp_means[3:4])
  
  wlp[i,]$score <- scoring(x=wlp[i,1:2], mu= as.matrix(mu_i),
                                    sigma=as.matrix(sig_c))
}

wlp$score %<>% as.factor()

ggplot()+
  geom_point(data=wlp, aes(x=Weight, y=Length, col=score), shape=1)+
  #geom_path(data=datac, aes(x=Weight, y=Length, group=prob))+
  theme_bw()+
  ylim(42.5, 55.1)+
  xlim(1922.5,4521)+
  theme( legend.position = "bottom") -> a2a



```
```{r p plot p25, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Plot of confidence ellipsoids and scatter plot with classification region for Weight and Lenght of newborn children under conditional distribution.", fig.height=3}
grid.arrange(a1, a2a, nrow=1)
```
We can see that the areas have decreased significantly, but the scoring system worked better.
In this case we award the kth child 2 points if:
$$(X_k-\hat{\mu}^*)^T\hat{\Sigma}^{*-1}(X_k-\hat{\mu}^*)\leq\chi_2^2(0.75),$$
1 point if:
$$\chi^2_2(0.75)\leq(X_k-\hat{\mu}^*)^T\hat{\Sigma}^{*-1}(X_k-\hat{\mu}^*)\leq\chi_2^2(0.95),$$
else 0.
In this case, there are 40 children with a score of 0, 137 with a score of 1, and 559 with a score of 2.
Confidence ellipses are not shown in a scatter plot showing classified points because for each point the pair of ellipses has a different center.

Below plot shows classification ellpsoids when $y=[185,178]^T$.

```{r code father, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Classification region for Weight and Lenght of newborn children, when father of a child is 185[cm] tall and mother is 178[cm] tall."}

wl_cov-wlp_cov[1:2,3:4]%*% solve(wlp_cov[3:4,3:4])%*% wlp_cov[3:4,1:2] -> sig_c
new_m<- (wl_means)+wlp_cov[1:2,3:4] %*% solve(wlp_cov[3:4,3:4]) %*% (c(185,178)-wlp_means[3:4])
new_m<- t(new_m)

ellipse_data_95f = ellipse(x = sig_c, centre = new_m , level = 0.95) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.95",100)))
ellipse_data_75f = ellipse(x = sig_c, centre = new_m , level = 0.75) %>% as.data.frame() %>% cbind(data.frame(prob=rep("0.75",100)))
datac<- rbind(ellipse_data_95f, ellipse_data_75f) 


ggplot(datac, aes(x=Weight, y=Length, colour=prob, group=prob)) +
  geom_path() +
  theme_bw()+
  theme(legend.position = "bottom")->a1
  

```

```{r p plot p2father5, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Classification region for Weight and Lenght of newborn children, when father of a child is 185[cm] tall and mother is 178[cm] tall.", , fig.height=3, fig.width=3}
a1
```

The eigenvalues of the matrix $\hat{\Sigma}'$ are $\lambda_1'=0.478$, $\lambda_2'=4.788$, $\lambda_3'=10.708$, $\lambda_4'=22.029$ and their corresponding eigenvectors, respectively: $v_1'=[0.005, -0.981, -0.122, -0.149]^T$, $v_2'=[0.005, 0.193, -0.564, -0.803]^T$, , $v_3'=[0.001,  0.014, -0.816,  0.577]^T$, $v_4'=[1,  0.004, 0.004, 0.004]^T$. The spectral decomposition of $\hat{\Sigma}$ has form:



\begin{equation*}
\begin{split}
\hat{\Sigma}&=P'\Lambda'P'^T\\
&=
\begin{bmatrix}
0.005 &0.005&  0.001&  1\\
 -0.981& 0.193& 0.0137&   0.004\\
 -0.122& -0.564&-0.816&  0.004\\
 -0.149&  -0.803& 0.577& 0.004\\
\end{bmatrix}
\begin{bmatrix}
0.478 &0&  0&  0\\
 0& 4.788& 0&   0\\
 0&  0&  10.708 & 0\\
 0& 0& 0& 22.029\\
\end{bmatrix}
\begin{bmatrix}
0.005& -0.981& -0.122& -0.149\\
0.005& 0.193& -0.564& -0.803\\
0.001&  0.014& -0.816&  0.577\\
 1&  0.004& 0.004& 0.004\\
\end{bmatrix}\\
&=
\begin{bmatrix}
220276.658& 915.296& 931.859& 827.288 \\
915.296&   4.443& 3.290&   2.852\\
931.859&   3.290&  12.612& 0.631\\
827.288&   2.852&  0.631&  9.772
\end{bmatrix}.
\end{split}
\end{equation*}


Let's transform the data as before, multiplying the data matrix on the left by $P'^T$.

```{r transformed data new, cache=TRUE, echo=FALSE, include=FALSE}
eig<- eigen(wlp_cov)
P<- eig$vectors[,4:1] %>% as.matrix()
data<- (t(P) %*% t(as.matrix(wlp[,1:4]) ) ) %>% t() %>% as.data.frame()
data<- data[,4:1]
colnames(data)<- c("Weight","Length", "FatherHeight", "MotherHeight")
```
```{r sctr qq plot newdata trans code, echo=FALSE, comment=F,  warning=FALSE, cache=TRUE, include=F}

ggplot(data, aes(x=Weight, y=Length))+
  geom_point(shape=1)+
  #ylim(158, 189)+
  theme_bw() ->a0

ggplot(data, aes(x=Weight, y=FatherHeight))+
  geom_point(shape=1)+
  #ylim(158, 189)+
  theme_bw() ->a1
ggplot(data, aes(x=Length, y=FatherHeight))+
  geom_point(shape=1)+
  #ylim(158, 189)+
  theme_bw() ->a2

ggplot(data, aes(x=Weight, y=MotherHeight))+
  geom_point(shape=1)+
  #ylim(158, 189)+
  theme_bw() ->a3
ggplot(data, aes(x=Length, y=MotherHeight))+
  geom_point(shape=1)+
  #ylim(158, 189)+
  theme_bw() ->a4
ggplot(data, aes(x=FatherHeight, y=MotherHeight))+
  geom_point(shape=1)+
  theme_bw() ->a5

ggarrange(a1,a2, nrow=1)-> a22
ggarrange(a3,a4, nrow=1)-> a23


```

```{r plot transformed data new transformed, echo=FALSE, comment=F, fig.align = "center", warning=FALSE, cache=TRUE, fig.cap="Scatterplot of transformed data (data of children and parents).", fig.height=8}
grid.arrange(a0, a22, a23, a5,nrow=4)
```




As before, the transformation reduced the dependency between the variables. In each of the above graphs, the points are arranged in groups that are not clustered around any straight lines.




